{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Process\n",
    "\n",
    "En este notebook realizaremos todo el Data Mining Process. En el apartado 2. se pueden ver los pasos a realizar:\n",
    "\n",
    "\n",
    "1. Entender el problema (Problem Understanding)\n",
    "2. Data Mining Process\n",
    "    * Obtener los datos (Data collect)\n",
    "    * Procesando los datos (Data Processing)\n",
    "    * Exploración estadística de datos (Statistical Data Exploration)\n",
    "    * Modelado (Analysis Modeling)\n",
    "3. Resultados (Results)\n",
    "\n",
    "\n",
    "\n",
    "Antes de nada, importamos las librerias y funciones necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import skmob\n",
    "from skmob.measures.individual import home_location\n",
    "from skmob.io.file import load_geolife_trajectories,read,write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1. Obtener los datos\n",
    "\n",
    "Antes de crear el modelo usando Tensorflow, es necesario cargar la información y preprocesarla para tenerla lista para entrenar el modelo de red neuronal que usaremos.\n",
    "\n",
    "Para cargar los datos, usamos la función 'load_geolife_trajectories' que nos proporciona skmob. Esta función se encarga de cargar los datos de GeoLife Trajectories, solo hay que indicarle el path del directorio Data, y los usuarios que queremos cargar. Es opcional pasarle algunos parámetros de filtrado y compresión.Esta función, por cómo esta implementada, solo contempla la sintaxis de las rutas de archivos en Linux, por lo que en windows no funcionará.\n",
    "\n",
    "Si queremos cargar/guardar estos datos, solo hay que hacer uso de las funciones read y write que proporciona skmob, que permiten escribir y cargar dataframes en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS DATOS YA ESTAN EN EL FICHERO\n",
    "# LOS DATOS YA ESTAN EN EL FICHERO\n",
    "# path = \"/home/alonso/Documentos/tfg/Geolife_Trajectories/\"\n",
    "# traj_df = load_geolife_trajectories(path, user_ids=[120,121,122,123],\n",
    "#                                    compress_kwargs={'spatial_radius_km': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path= \"../../Data/gps_data.json\"\n",
    "traj_df = read(json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Procesando los datos\n",
    "\n",
    "En esta segunda fase, es importante tener una \"visión global\" de los datos, con el fin de entenderlos mejor. Una buena idea es ver si hay datos nulos en la información que tenemos, ya que estos deberían ser tratados. Se puede ver que en esta ocasión, no los hay. Si los hubiese, una opción podría ser la de descartar estos datos. Para realizar esto último, se podría hacer uso del método dropna() para descartar las filas con algún dato nulo los datos. Otra opcion sería remplazar los nulos por algun otro valor con fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valores nulos de cada columna: \n",
      "lat         0\n",
      "lng         0\n",
      "datetime    0\n",
      "uid         0\n",
      "dtype: int64\n",
      "\n",
      "Vistazo de los datos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>datetime</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.982375</td>\n",
       "      <td>116.320442</td>\n",
       "      <td>2009-09-19 07:11:37</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.982380</td>\n",
       "      <td>116.320448</td>\n",
       "      <td>2009-09-19 07:11:48</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.982380</td>\n",
       "      <td>116.320455</td>\n",
       "      <td>2009-09-19 07:11:50</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.982397</td>\n",
       "      <td>116.320455</td>\n",
       "      <td>2009-09-19 07:11:52</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.982425</td>\n",
       "      <td>116.320460</td>\n",
       "      <td>2009-09-19 07:11:54</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat         lng            datetime  uid\n",
       "0  39.982375  116.320442 2009-09-19 07:11:37  120\n",
       "1  39.982380  116.320448 2009-09-19 07:11:48  120\n",
       "2  39.982380  116.320455 2009-09-19 07:11:50  120\n",
       "3  39.982397  116.320455 2009-09-19 07:11:52  120\n",
       "4  39.982425  116.320460 2009-09-19 07:11:54  120"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(traj_df)\n",
    "print('Total de valores nulos de cada columna: ')\n",
    "print(df.isna().sum() )\n",
    "print('\\nVistazo de los datos:')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la localización de las casas de cada individuo en el dataset, scikit-mobility nos proporciona la función home_location(). Esta función observa la localización más frecuentada en la noche (Entre las 22:00 y las 07:00) por los usuarios y designa a este como su casa. Estas horas son modificables, aunque no lo haremos ya que lo normal, es estar en casa entre esas horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 135.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>42.405338</td>\n",
       "      <td>117.249225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>39.906183</td>\n",
       "      <td>116.379599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122</td>\n",
       "      <td>39.968092</td>\n",
       "      <td>116.399647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>34.264762</td>\n",
       "      <td>108.939026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid        lat         lng\n",
       "0  120  42.405338  117.249225\n",
       "1  121  39.906183  116.379599\n",
       "2  122  39.968092  116.399647\n",
       "3  123  34.264762  108.939026"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_loc_df = home_location(df)\n",
    "home_loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya tenemos los dos dataframes, uno con los datos GPS y otro con las localizaciones de las casas de cada individuo, es necesario trabajar con ellos para obtener los datos que buscamos.\n",
    "\n",
    "Añadimos al dataframe de los datos GPS (nuestro 'df') una columna extra que indicará si esta en casa y la inicializamos con 0. Al dataframe con la localizacion de las casas de los individuos (home_loc_df) le añadimos una columna extra de mismo nombre, con valor 1, ya que en esa posición se encuentra la casa.\n",
    "\n",
    "Despues, hacemos lo que en base de datos se conoce como un JOIN LEFT usando como key los valores 'lat', 'lng' y 'uid'. Con conseguiremos tener la una columna at_home_y, con el valor de 1 cuando se encuentre una coincidencia y un valor nulo cuando no y otra at_home_x con todos sus valores 0. \n",
    "\n",
    "Por último ya solo quedaría crear y asignar a una nueva columna at_home el valor de la columna at_home_y y rellenar los valores nulos con los de la columna at_home_x (que tienen valor 0). Despues ya solo quedaría descartar las columnas que sobran y quedarmos con at_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>datetime</th>\n",
       "      <th>uid</th>\n",
       "      <th>at_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.982375</td>\n",
       "      <td>116.320442</td>\n",
       "      <td>2009-09-19 07:11:37</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.982380</td>\n",
       "      <td>116.320448</td>\n",
       "      <td>2009-09-19 07:11:48</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.982380</td>\n",
       "      <td>116.320455</td>\n",
       "      <td>2009-09-19 07:11:50</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.982397</td>\n",
       "      <td>116.320455</td>\n",
       "      <td>2009-09-19 07:11:52</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.982425</td>\n",
       "      <td>116.320460</td>\n",
       "      <td>2009-09-19 07:11:54</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat         lng            datetime  uid  at_home\n",
       "0  39.982375  116.320442 2009-09-19 07:11:37  120      0.0\n",
       "1  39.982380  116.320448 2009-09-19 07:11:48  120      0.0\n",
       "2  39.982380  116.320455 2009-09-19 07:11:50  120      0.0\n",
       "3  39.982397  116.320455 2009-09-19 07:11:52  120      0.0\n",
       "4  39.982425  116.320460 2009-09-19 07:11:54  120      0.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['at_home'] = 0\n",
    "home_loc_df['at_home'] = 1\n",
    "df = pd.merge(df, home_loc_df, on=['lat','lng','uid'], how='left')\n",
    "df['at_home'] = df['at_home_y'].fillna(df['at_home_x'])  \n",
    "df = df.drop(['at_home_x','at_home_y'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Exploración estadística de datos\n",
    "\n",
    "Observaremos las estadísticas de los datos y  comprobaremos si hay correlaciones entre ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat                float64\n",
      "lng                float64\n",
      "datetime    datetime64[ns]\n",
      "uid                  int64\n",
      "at_home            float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>uid</th>\n",
       "      <th>at_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92830.000000</td>\n",
       "      <td>92830.000000</td>\n",
       "      <td>92830.000000</td>\n",
       "      <td>92830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.559663</td>\n",
       "      <td>115.764088</td>\n",
       "      <td>121.804212</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.675042</td>\n",
       "      <td>2.255154</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>0.010885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.534653</td>\n",
       "      <td>108.726840</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.964671</td>\n",
       "      <td>116.231612</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.976559</td>\n",
       "      <td>116.398177</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.131145</td>\n",
       "      <td>116.474284</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.551380</td>\n",
       "      <td>117.957961</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lat           lng           uid       at_home\n",
       "count  92830.000000  92830.000000  92830.000000  92830.000000\n",
       "mean      38.559663    115.764088    121.804212      0.000118\n",
       "std        4.675042      2.255154      0.751236      0.010885\n",
       "min       22.534653    108.726840    120.000000      0.000000\n",
       "25%       39.964671    116.231612    122.000000      0.000000\n",
       "50%       39.976559    116.398177    122.000000      0.000000\n",
       "75%       40.131145    116.474284    122.000000      0.000000\n",
       "max       42.551380    117.957961    123.000000      1.000000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "at_home    1.000000\n",
       "lat        0.001342\n",
       "lng       -0.003134\n",
       "Name: at_home, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix =  df.corr()\n",
    "matrix['at_home'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Modelado\n",
    "\n",
    "Antes de crear el modelo de red neuronal con Tensorflow, es necesario normalizar las columnas, esto se hace para impedir que alguna feature influya más que otra en la predicción. Además, el hecho de tener los datos en valores entre 0 y 1, facilita bastante el entrenamiento. Para esto nos ayudaremos de sklearn y su método MinMaxScaler. mostando el dataframe nos damos cuenta que los datos se han normalizado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TO_DO: ENCONTRAR MANERA DE PREPROCESAR DATETIMES\n",
    "## TO_DO: ENCONTRAR MANERA DE PREPROCESAR DATETIMES\n",
    "df_preprocesed = df.drop('datetime', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>uid</th>\n",
       "      <th>at_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92825</th>\n",
       "      <td>0.584057</td>\n",
       "      <td>0.024237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92826</th>\n",
       "      <td>0.584059</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92827</th>\n",
       "      <td>0.584061</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92828</th>\n",
       "      <td>0.584074</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92829</th>\n",
       "      <td>0.584073</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lat       lng  uid  at_home\n",
       "92825  0.584057  0.024237  1.0      0.0\n",
       "92826  0.584059  0.024239  1.0      0.0\n",
       "92827  0.584061  0.024239  1.0      0.0\n",
       "92828  0.584074  0.024239  1.0      0.0\n",
       "92829  0.584073  0.024239  1.0      0.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_preprocesed) \n",
    "df_preprocesed.loc[:,:] = scaled_values\n",
    "df_preprocesed.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos dividirlos en el set de entrenamiento y en el set de test, guardando la misma proporción de valores de la columna 'at_home' en ambos sets.\n",
    "\n",
    "Para esto nos podemos ayudar de sklearn y su método StratifiedShuffleSplit. Para verificar que los datos se han separado en la misma proporción, basta con dividir tanto el número de 0 como de 1 entre el total de datos de cada set respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=10)\n",
    "\n",
    "for train_index, test_index in split.split(df_preprocesed, df_preprocesed[\"at_home\"]):\n",
    "    strat_train_set = df_preprocesed.loc[train_index]\n",
    "    strat_test_set = df_preprocesed.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.999892\n",
       "1.0    0.000108\n",
       "Name: at_home, dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set[\"at_home\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.999879\n",
       "1.0    0.000121\n",
       "Name: at_home, dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set[\"at_home\"].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora debemos separar la columna 'at_home' del resto de features en ambos conjuntos, ya que es la columna a predecir (nuestro target). Para ello usaremos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = strat_train_set\n",
    "test_set = strat_test_set\n",
    "train_labels = train_set.pop('at_home')\n",
    "test_labels = test_set.pop('at_home')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el modelo de Tensorflow 2.0 usando la API de alto nivel que incorpora llamada keras:\n",
    "\n",
    "* La input layer no es una capa en sí, sinó un tensor, concretamente el tensor inicial que se manda a la primera capa oculta. \n",
    "* El tensor debe tener la misma shape(forma) que nuestros datos. En nuestro caso, este tensor es de 1D, con con tamaño igual al número de features de nuestro set de nuestro set de entrenamiento 'train_set'\n",
    "* Se utilizarán dos hidden layer (hidden layer 1 y hidden layer 2) con 32 neuronas cada una, utilizando la función de activación 'relu'\n",
    "* En cuanto a la ouput layer o capa de salida, estará compuesta de 1 neurona, ya que solo queremos predecir un dato. Como estamos haciendo una clasificación binaria, deberemos usar la función de activación 'sigmoid' que dará valores entre 0 y 1, lo que nos viene perfecto. \n",
    "* La función de perdida será la 'binary_crossentropy', que es la usada en problemas de clasificación binaria.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![red_neuronal_ejemplo](../../Images/neural_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=[len(train_set.keys())]),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer un resumen del modelo, podemos usar la función summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,217\n",
      "Trainable params: 1,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos el modelo durante 5 epochs y veremos como evoluciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51984 samples, validate on 22280 samples\n",
      "Epoch 1/5\n",
      "51984/51984 [==============================] - 7s 126us/sample - loss: 0.0153 - accuracy: 0.9998 - val_loss: 5.0296e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "51984/51984 [==============================] - 6s 109us/sample - loss: 0.0016 - accuracy: 0.9998 - val_loss: 4.8529e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "51984/51984 [==============================] - 6s 107us/sample - loss: 0.0016 - accuracy: 0.9998 - val_loss: 4.7882e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "51984/51984 [==============================] - 7s 130us/sample - loss: 0.0016 - accuracy: 0.9998 - val_loss: 4.6338e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "51984/51984 [==============================] - 6s 123us/sample - loss: 0.0015 - accuracy: 0.9998 - val_loss: 4.6936e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_set, \n",
    "    train_labels,\n",
    "    epochs=5, \n",
    "    validation_split = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015276</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0  0.015276  0.999846  0.000503      0.999955      0\n",
       "1  0.001623  0.999846  0.000485      0.999955      1\n",
       "2  0.001642  0.999846  0.000479      0.999955      2\n",
       "3  0.001617  0.999846  0.000463      0.999955      3\n",
       "4  0.001548  0.999846  0.000469      0.999955      4"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo en el test de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18566/18566 - 1s - loss: 0.0013 - accuracy: 0.9999\n",
      "\n",
      "Test accuracy: 0.9998923\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
